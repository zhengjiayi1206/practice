#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Flappy Bird Double DQN - PyTorch ç‰ˆæœ¬

ä¾èµ–å»ºè®®ï¼š
- python == 3.10
- torch  >= 2.0 (ä½ å¯ä»¥ç”¨ 2.2.1ï¼›å¦‚æœ 3.10 ä¸Šè£…ä¸åˆ° 2.3.0 å°±ç”¨ 2.2.1)
- numpy == 1.26.4
- opencv-python == 4.9.0 æˆ– 4.8.x
- pygame == 2.5.2

ç¯å¢ƒï¼š
- ä½¿ç”¨åŸæ¥çš„ game/wrapped_flappy_bird.py é‡Œçš„ GameState()
"""

from __future__ import print_function
import os
import sys
import cv2
import time
import math
import random
import numpy as np
from collections import deque

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

sys.path.append("game/")
from wrapped_flappy_bird import GameState

# ==================== è¶…å‚æ•° ====================
GAME = "bird"
ACTIONS = 2                 # åŠ¨ä½œæ•°ï¼šä¸è·³ / è·³
GAMMA = 0.99               # æŠ˜æ‰£å› å­
OBSERVE = 1000             # çº¯è§‚å¯Ÿæ­¥æ•°ï¼ˆåªæ”¶é›†ç»éªŒï¼Œä¸è®­ç»ƒï¼‰
EXPLORE = 300000           # epsilon ä» INITIAL è¡°å‡åˆ° FINAL æ‰€èŠ±çš„æ­¥æ•°
INITIAL_EPSILON = 0.5
FINAL_EPSILON = 0.05
REPLAY_MEMORY = 100000     # ç»éªŒæ± å®¹é‡
BATCH = 32
FRAME_PER_ACTION = 4
LEARNING_RATE = 2.5e-4
TARGET_UPDATE_FREQ = 2000  # target ç½‘ç»œåŒæ­¥é¢‘ç‡ï¼ˆæ­¥æ•°ï¼‰
SAVE_INTERVAL = 20000      # ä¿å­˜æ¨¡å‹çš„æ­¥æ•°é—´éš”

# ==================== è®¾å¤‡é€‰æ‹© ====================
device = torch.device(
    "mps" if torch.backends.mps.is_available()
    else ("cuda" if torch.cuda.is_available() else "cpu")
)
print("Using device:", device)


# ==================== ç½‘ç»œç»“æ„ ====================
class DQN(nn.Module):
    """
    CNN ç»“æ„å°½é‡è´´è¿‘ä½ åŸæ¥çš„ TF ç‰ˆï¼š
    - Conv1: 4 â†’ 32, kernel=8, stride=4, padding=2
    - MaxPool: 2x2, stride=2
    - Conv2: 32 â†’ 64, kernel=4, stride=2, padding=1
    - Conv3: 64 â†’ 64, kernel=3, stride=1, padding=1
    - Flatten åå°ºå¯¸ä¸º 64 * 5 * 5 = 1600
    - FC1: 1600 -> 512
    - FC2: 512 -> ACTIONS
    """

    def __init__(self, num_actions=ACTIONS):
        super(DQN, self).__init__()
        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=2)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)

        # åŠ¨æ€ç®— flatten sizeï¼Œé¿å…ç¡¬ç¼–ç é”™è¯¯
        with torch.no_grad():
            dummy = torch.zeros(1, 4, 80, 80)
            x = self.conv1(dummy)
            x = self.pool1(x)
            x = self.conv2(x)
            x = self.conv3(x)
            self.flatten_size = x.view(1, -1).size(1)
        # ä¸€èˆ¬åº”è¯¥æ˜¯ 1600
        print("Flatten size =", self.flatten_size)

        self.fc1 = nn.Linear(self.flatten_size, 512)
        self.fc2 = nn.Linear(512, num_actions)

    def forward(self, x):
        # x: [B, 4, 80, 80], åƒç´  0~255
        x = x / 255.0  # å½’ä¸€åŒ–
        x = F.relu(self.conv1(x))
        x = self.pool1(x)
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        q_values = self.fc2(x)  # [B, ACTIONS]
        return q_values


# ==================== Replay Memory ====================
class ReplayMemory(object):
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        # state / next_state: np.array, å½¢çŠ¶ [4, 80, 80]
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        minibatch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*minibatch)
        return (
            np.stack(states, axis=0),
            np.array(actions, dtype=np.int64),
            np.array(rewards, dtype=np.float32),
            np.stack(next_states, axis=0),
            np.array(dones, dtype=np.float32),
        )

    def __len__(self):
        return len(self.buffer)


# ==================== å›¾åƒé¢„å¤„ç† ====================
def preprocess(frame):
    """
    è¾“å…¥ï¼šRGB frame (H, W, 3)
    è¾“å‡ºï¼šç°åº¦äºŒå€¼åŒ– (80, 80), float32
    """
    frame = cv2.cvtColor(cv2.resize(frame, (80, 80)), cv2.COLOR_BGR2GRAY)
    _, frame = cv2.threshold(frame, 128, 255, cv2.THRESH_BINARY)
    return frame.astype(np.float32)


# ==================== è®­ç»ƒä¸»å¾ªç¯ ====================
def train():
    env = GameState()

    # ä¸»ç½‘ç»œ & ç›®æ ‡ç½‘ç»œ
    main_net = DQN().to(device)
    target_net = DQN().to(device)
    target_net.load_state_dict(main_net.state_dict())
    target_net.eval()

    optimizer = optim.Adam(main_net.parameters(), lr=LEARNING_RATE)
    memory = ReplayMemory(REPLAY_MEMORY)

    print("ğŸš€ Double DQN training started!")

    # åˆå§‹çŠ¶æ€ï¼šdo nothing
    do_nothing = np.zeros(ACTIONS)
    do_nothing[0] = 1
    frame, _, terminal = env.frame_step(do_nothing)

    frame = preprocess(frame)
    # åˆå§‹ state: 4 å¸§ç›¸åŒ
    state = np.stack([frame] * 4, axis=0)  # [4,80,80]

    epsilon = INITIAL_EPSILON
    t = 0  # å…¨å±€æ­¥æ•°

    while True:
        # ========== 1. é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-greedyï¼‰ ==========
        state_tensor = torch.from_numpy(state).unsqueeze(0).to(device)  # [1,4,80,80]
        q_values = main_net(state_tensor)
        q_values_np = q_values.detach().cpu().numpy()[0]

        action_index = 0
        action_onehot = np.zeros(ACTIONS)

        if t % FRAME_PER_ACTION == 0:
            if random.random() <= epsilon:
                # æ¢ç´¢ï¼šå‡å°‘éšæœºè·³çš„æ¯”ä¾‹
                jump_random_prob = 0.20
                if random.random() < jump_random_prob:
                    action_index = 1  # è·³
                else:
                    action_index = 0  # ä¸è·³
            else:
                # åˆ©ç”¨ï¼šé€‰ Q æœ€å¤§çš„åŠ¨ä½œ
                action_index = int(np.argmax(q_values_np))
        else:
            # éåŠ¨ä½œå¸§ï¼šé»˜è®¤ä¸è·³
            action_index = 0

        action_onehot[action_index] = 1

        # epsilon çº¿æ€§é€€ç«
        if t > OBSERVE and epsilon > FINAL_EPSILON:
            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / float(EXPLORE)
            epsilon = max(FINAL_EPSILON, epsilon)

        # ========== 2. æ‰§è¡ŒåŠ¨ä½œï¼Œå¾—åˆ°æ–°å¸§å’Œ reward ==========
        next_frame_color, r, done = env.frame_step(action_onehot)

        # å¥–åŠ±æ•´å½¢
        if done:
            r = -1.0
        elif r == 1:
            r = 1.0
        else:
            r = 0.0002

        next_frame = preprocess(next_frame_color)
        next_state = np.concatenate(
            ([next_frame], state[:-1]), axis=0
        )  # æ–°å¸§åœ¨æœ€å‰ [4,80,80]

        # ========== 3. å­˜å…¥ç»éªŒæ±  ==========
        memory.push(state, action_index, r, next_state, done)
        state = next_state
        t += 1

        # ========== 4. ä»ç»éªŒæ± é‡‡æ ·å¹¶è®­ç»ƒï¼ˆDouble DQN æ ¸å¿ƒï¼‰ ==========
        if t > OBSERVE and len(memory) >= BATCH:
            (
                batch_states,
                batch_actions,
                batch_rewards,
                batch_next_states,
                batch_dones,
            ) = memory.sample(BATCH)

            # è½¬æˆ tensor
            batch_states_t = torch.from_numpy(batch_states).to(device)          # [B,4,80,80]
            batch_actions_t = torch.from_numpy(batch_actions).to(device)        # [B]
            batch_rewards_t = torch.from_numpy(batch_rewards).to(device)        # [B]
            batch_next_states_t = torch.from_numpy(batch_next_states).to(device)# [B,4,80,80]
            batch_dones_t = torch.from_numpy(batch_dones).to(device)            # [B]

            # --- Q(s,a) from main_net ---
            q_values = main_net(batch_states_t)                 # [B,2]
            # é€‰å‡ºå¯¹åº”åŠ¨ä½œçš„ Q(s,a)
            q_selected = q_values.gather(1, batch_actions_t.unsqueeze(1)).squeeze(1)

            # --- Double DQN: ä¸»ç½‘é€‰ a_maxï¼Œç›®æ ‡ç½‘è¯„ä¼° ---
            # ä¸»ç½‘ç»œåœ¨ s' ä¸Šé€‰åŠ¨ä½œ
            q_next_main = main_net(batch_next_states_t)         # [B,2]
            next_actions = q_next_main.argmax(dim=1)            # [B]

            # ç›®æ ‡ç½‘ç»œåœ¨ s' ä¸Šè¯„ä¼°è¿™äº›åŠ¨ä½œ
            q_next_target = target_net(batch_next_states_t)     # [B,2]
            q_next_selected = q_next_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)

            # å¯¹ç»ˆæ­¢çŠ¶æ€ï¼Œæœªæ¥å›æŠ¥ä¸º 0
            targets = batch_rewards_t + GAMMA * q_next_selected * (1.0 - batch_dones_t)

            # Huber lossï¼ˆSmooth L1ï¼‰
            loss = F.smooth_l1_loss(q_selected, targets.detach())

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        # ========== 5. æ›´æ–°ç›®æ ‡ç½‘ç»œ ==========
        if t % TARGET_UPDATE_FREQ == 0:
            target_net.load_state_dict(main_net.state_dict())
            print("Target network updated at step", t)

        # ========== 6. ä¿å­˜æ¨¡å‹ ==========
        if t % SAVE_INTERVAL == 0:
            os.makedirs("saved_networks", exist_ok=True)
            save_path = os.path.join("saved_networks", f"{GAME}-double-dqn-{t}.pth")
            torch.save(main_net.state_dict(), save_path)
            print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜:", save_path, "| å½“å‰ Îµ = {:.3f}".format(epsilon))

        # ========== 7. æ‰“å°è®­ç»ƒçŠ¶æ€ ==========
        if t <= OBSERVE:
            phase = "observe"
        elif t <= OBSERVE + EXPLORE:
            phase = "explore"
        else:
            phase = "train"

        print(
            "Step {} | {} | Îµ = {:.3f} | Action = {} | Reward = {:.4f}".format(
                t, phase, epsilon, action_index, r
            )
        )


if __name__ == "__main__":
    train()